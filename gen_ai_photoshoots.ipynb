{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theskrtnerd/gen-ai-photoshoots/blob/main/gen_ai_photoshoots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "me1dBGCvjZgj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install streamlit python-dotenv google-generativeai torch diffusers transformers accelerate torchvision bitsandbytes datasets pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LhoLTaIGxSn",
        "outputId": "3b4fbd4d-f61c-4d6f-d6b2-d67649019edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting download_model.py\n"
          ]
        }
      ],
      "source": [
        "#@title Writing download_model.py\n",
        "%%writefile download_model.py\n",
        "import os\n",
        "\n",
        "\n",
        "def download_model(my_bar):\n",
        "    \"\"\"\n",
        "    Download and save the pretrained Stable Diffusion model components.\n",
        "\n",
        "    This function downloads the tokenizer, text encoder, variational autoencoder (VAE),\n",
        "    U-Net, and feature extractor components of the Stable Diffusion model and saves them\n",
        "    to a specified directory. It also updates a progress bar to reflect the download progress.\n",
        "\n",
        "    Args:\n",
        "        my_bar: A Streamlit progress bar object used to update the download progress.\n",
        "\n",
        "    The function performs the following steps:\n",
        "        1. Checks if the save directory exists; if not, creates it.\n",
        "        2. Downloads the tokenizer and updates the progress bar to 20%.\n",
        "        3. Downloads the text encoder and updates the progress bar to 40%.\n",
        "        4. Downloads the VAE and updates the progress bar to 60%.\n",
        "        5. Downloads the U-Net and updates the progress bar to 80%.\n",
        "        6. Downloads the feature extractor and updates the progress bar to 96%.\n",
        "        7. Saves each downloaded component to the specified directory.\n",
        "    \"\"\"\n",
        "    save_directory = \"./stable_diffusion_models\"\n",
        "\n",
        "    if not os.path.exists(save_directory):\n",
        "        os.makedirs(save_directory)\n",
        "        my_bar.progress(0, \"Downloading the pretrained model...\")\n",
        "        from transformers import CLIPTokenizer, CLIPTextModel, CLIPFeatureExtractor\n",
        "        from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "\n",
        "        # The Stable Diffusion checkpoint we'll fine-tune\n",
        "        model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "        tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
        "        my_bar.progress(20, \"Downloading the pretrained model...\")\n",
        "        text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
        "        my_bar.progress(40, \"Downloading the pretrained model...\")\n",
        "        vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
        "        my_bar.progress(60, \"Downloading the pretrained model...\")\n",
        "        unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
        "        my_bar.progress(80, \"Downloading the pretrained model...\")\n",
        "        feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        my_bar.progress(96, \"Downloading the pretrained model...\")\n",
        "\n",
        "        # Save each component\n",
        "        tokenizer.save_pretrained(f\"{save_directory}/tokenizer\")\n",
        "        text_encoder.save_pretrained(f\"{save_directory}/text_encoder\")\n",
        "        vae.save_pretrained(f\"{save_directory}/vae\")\n",
        "        unet.save_pretrained(f\"{save_directory}/unet\")\n",
        "        feature_extractor.save_pretrained(f\"{save_directory}/feature_extractor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xJj44PgG3_O",
        "outputId": "53cf1395-5b7d-41f7-fe58-7f1e327f64b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_model.py\n"
          ]
        }
      ],
      "source": [
        "#@title Writing train_model.py\n",
        "%%writefile train_model.py\n",
        "from argparse import Namespace\n",
        "from accelerate import Accelerator, notebook_launcher\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from download_model import download_model\n",
        "\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class for preparing data for training the Stable Diffusion model.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset containing images.\n",
        "        instance_prompt (str): The prompt describing the instance in the images.\n",
        "        tokenizer (CLIPTokenizer): The tokenizer for encoding text prompts.\n",
        "        size (int, optional): The size to which images are resized. Default is 512.\n",
        "\n",
        "    Methods:\n",
        "        __len__(): Returns the length of the dataset.\n",
        "        __getitem__(index): Returns a single data point (image and tokenized prompt).\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, instance_prompt, tokenizer, size=512):\n",
        "        self.dataset = dataset\n",
        "        self.instance_prompt = instance_prompt\n",
        "        self.tokenizer = tokenizer\n",
        "        self.size = size\n",
        "        self.transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size),\n",
        "                transforms.CenterCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        # Assuming self.dataset is a list of dictionaries, each containing an \"image\" key\n",
        "        image = self.dataset[index][\"image\"]\n",
        "        # Apply transforms to the image\n",
        "        example[\"instance_images\"] = self.transforms(image)\n",
        "        # Assuming self.instance_prompt is defined somewhere in your class\n",
        "        # and tokenizer is a CLIPTokenizer instance\n",
        "\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        )[\"input_ids\"]\n",
        "        return example\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    \"\"\"\n",
        "    Collate function to prepare a batch of data for training.\n",
        "\n",
        "    Args:\n",
        "        examples (list): A list of examples where each example is a dictionary\n",
        "                         containing 'instance_prompt_ids' and 'instance_images'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing batched 'input_ids' and 'pixel_values'.\n",
        "    \"\"\"\n",
        "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "    pixel_values = torch.stack(pixel_values)\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format)\n",
        "    pixel_values = pixel_values.float()\n",
        "\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\"./stable_diffusion_models/tokenizer\")\n",
        "\n",
        "    # Tokenize prompts\n",
        "    batch_encoding = tokenizer.pad(\n",
        "        {\"input_ids\": input_ids}, padding=True, return_attention_mask=True, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Extract input_ids and attention_mask from batch_encoding\n",
        "    input_ids = batch_encoding[\"input_ids\"]\n",
        "    attention_mask = batch_encoding[\"attention_mask\"]\n",
        "\n",
        "    batch = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"pixel_values\": pixel_values,\n",
        "    }\n",
        "    return batch\n",
        "\n",
        "\n",
        "def training_function(text_encoder, vae, unet, args, my_bar):\n",
        "    \"\"\"\n",
        "    Function to train the Stable Diffusion model.\n",
        "\n",
        "    Args:\n",
        "        text_encoder (CLIPTextModel): The text encoder model.\n",
        "        vae (AutoencoderKL): The variational autoencoder model.\n",
        "        unet (UNet2DConditionModel): The U-Net model.\n",
        "        args (Namespace): A Namespace object containing training arguments and configurations.\n",
        "        my_bar (streamlit.progress): A Streamlit progress bar to monitor the training progress.\n",
        "\n",
        "    This function performs the following steps:\n",
        "        1. Sets up the training environment and configurations.\n",
        "        2. Loads the data and prepares it for training.\n",
        "        3. Defines the optimizer and noise scheduler.\n",
        "        4. Trains the model for the specified number of steps, updating the progress bar.\n",
        "        5. Saves the trained model pipeline.\n",
        "    \"\"\"\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\"./stable_diffusion_models/tokenizer\")\n",
        "    feature_extractor = CLIPFeatureExtractor.from_pretrained(\"./stable_diffusion_models/feature_extractor\")\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "\n",
        "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
        "    if args.use_8bit_adam:\n",
        "        import bitsandbytes as bnb\n",
        "        optimizer_class = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_class = torch.optim.AdamW\n",
        "\n",
        "    optimizer = optimizer_class(\n",
        "        unet.parameters(),  # Only optimize unet\n",
        "        lr=args.learning_rate,\n",
        "    )\n",
        "\n",
        "    noise_scheduler = DDPMScheduler(\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000,\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        args.train_dataset,\n",
        "        batch_size=args.train_batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "    unet, optimizer, train_dataloader = accelerator.prepare(\n",
        "        unet, optimizer, train_dataloader\n",
        "    )\n",
        "\n",
        "    # Move text_encode and vae to gpu\n",
        "    text_encoder.to(accelerator.device)\n",
        "    vae.to(accelerator.device)\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed\n",
        "    num_update_steps_per_epoch = math.ceil(\n",
        "        len(train_dataloader) / args.gradient_accumulation_steps\n",
        "    )\n",
        "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # Only show the progress bar once on each machine\n",
        "    progress_bar = tqdm(\n",
        "        range(args.max_train_steps), disable=not accelerator.is_local_main_process\n",
        "    )\n",
        "    progress_bar.set_description(\"Steps\")\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_train_epochs):\n",
        "        unet.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Convert images to latent space\n",
        "                with torch.no_grad():\n",
        "                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
        "                    latents = latents * 0.18215\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn(latents.shape).to(latents.device)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(\n",
        "                    0,\n",
        "                    noise_scheduler.config.num_train_timesteps,\n",
        "                    (bsz,),\n",
        "                    device=latents.device,\n",
        "                ).long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                with torch.no_grad():\n",
        "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                # Predict the noise residual\n",
        "                noise_pred = unet(\n",
        "                    noisy_latents, timesteps, encoder_hidden_states\n",
        "                ).sample\n",
        "                loss = (\n",
        "                    F.mse_loss(noise_pred, noise, reduction=\"none\")\n",
        "                    .mean([1, 2, 3])\n",
        "                    .mean()\n",
        "                )\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item()}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            my_bar.progress(int(epoch*100/num_train_epochs), text=\"Training in progress...\")\n",
        "\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "        accelerator.wait_for_everyone()\n",
        "\n",
        "    # Create the pipeline using the trained modules and save it\n",
        "    if accelerator.is_main_process:\n",
        "        print(f\"Loading pipeline and saving to {args.output_dir}...\")\n",
        "        scheduler = PNDMScheduler(\n",
        "            beta_start=0.00085,\n",
        "            beta_end=0.012,\n",
        "            beta_schedule=\"scaled_linear\",\n",
        "            skip_prk_steps=True,\n",
        "            steps_offset=1,\n",
        "        )\n",
        "        pipeline = StableDiffusionPipeline(\n",
        "            text_encoder=text_encoder,\n",
        "            vae=vae,\n",
        "            unet=accelerator.unwrap_model(unet),\n",
        "            tokenizer=tokenizer,\n",
        "            scheduler=scheduler,\n",
        "            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\n",
        "                \"CompVis/stable-diffusion-safety-checker\"\n",
        "            ),\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "\n",
        "\n",
        "def train_model(product_name, my_bar):\n",
        "    \"\"\"\n",
        "    Train the Stable Diffusion model with images of the specified product.\n",
        "\n",
        "    Args:\n",
        "        product_name (str): The name of the product to be used in the training prompt.\n",
        "        my_bar (streamlit.progress): A Streamlit progress bar to monitor the training progress.\n",
        "\n",
        "    This function performs the following steps:\n",
        "        1. Downloads the pretrained model components.\n",
        "        2. Loads the dataset containing images of the product.\n",
        "        3. Creates a DreamBoothDataset with the loaded images and the product name.\n",
        "        4. Sets up training arguments and configurations.\n",
        "        5. Initializes the model components.\n",
        "        6. Launches the training process using the specified number of GPUs.\n",
        "        7. Saves the trained model pipeline.\n",
        "    \"\"\"\n",
        "    download_model(my_bar)\n",
        "    dataset = load_dataset(\"imagefolder\", data_dir=\"training_photos/\", split='train')\n",
        "    instance_prompt = f\"a photo of a {product_name}\"\n",
        "    learning_rate = 2e-06\n",
        "    max_train_steps = 200\n",
        "    load_directory = \"./stable_diffusion_models\"\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(f\"{load_directory}/tokenizer\")\n",
        "    train_dataset = DreamBoothDataset(dataset, instance_prompt, tokenizer)\n",
        "    args = Namespace(\n",
        "        pretrained_model_name_or_path=load_directory,\n",
        "        resolution=512,  # Reduce this if you want to save some memory\n",
        "        train_dataset=train_dataset,\n",
        "        instance_prompt=instance_prompt,\n",
        "        learning_rate=learning_rate,\n",
        "        max_train_steps=max_train_steps,\n",
        "        train_batch_size=1,\n",
        "        gradient_accumulation_steps=1,  # Increase this if you want to lower memory usage\n",
        "        max_grad_norm=1.0,\n",
        "        gradient_checkpointing=True,  # Set this to True to lower the memory usage\n",
        "        use_8bit_adam=True,  # Use 8bit optimizer from bitsandbytes\n",
        "        seed=3434554,\n",
        "        sample_batch_size=2,\n",
        "        output_dir=\"./pipeline-folder\",  # Where to save the pipeline\n",
        "    )\n",
        "    text_encoder = CLIPTextModel.from_pretrained(f\"{load_directory}/text_encoder\")\n",
        "    vae = AutoencoderKL.from_pretrained(f\"{load_directory}/vae\")\n",
        "    unet = UNet2DConditionModel.from_pretrained(f\"{load_directory}/unet\")\n",
        "    num_of_gpus = 1  # CHANGE THIS TO MATCH THE NUMBER OF GPUS YOU HAVE\n",
        "    notebook_launcher(\n",
        "        training_function, args=(text_encoder, vae, unet, args, my_bar), num_processes=num_of_gpus\n",
        "    )\n",
        "    my_bar.progress(96, text=\"Saving the model...\")\n",
        "    with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXp6VkqKeuCw",
        "outputId": "21e9723e-24b1-4ad3-fc28-4ea61c0f2af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting generate_image.py\n"
          ]
        }
      ],
      "source": [
        "#@title Writing generate_image.py\n",
        "%%writefile generate_image.py\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Generate an image based on the provided prompt using the Stable Diffusion model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The text prompt describing the desired image.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: The generated image based on the prompt.\n",
        "\n",
        "    This function performs the following steps:\n",
        "        1. Loads the Stable Diffusion pipeline from the pretrained model directory.\n",
        "        2. Moves the pipeline to the GPU for faster processing.\n",
        "        3. Sets the guidance scale to influence the strength of the prompt in the generation process.\n",
        "        4. Generates the image using the provided prompt.\n",
        "        5. Returns the generated image.\n",
        "    \"\"\"\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"./pipeline-folder\",\n",
        "        torch_dtype=torch.float16,\n",
        "    ).to(\"cuda\")\n",
        "    guidance_scale = 9\n",
        "    image = pipe(prompt, guidance_scale=guidance_scale).images\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Writing gemini_api.py\n",
        "%%writefile gemini_api.py\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import json\n",
        "\n",
        "\n",
        "def validate_gemini_api_key(my_api_key):\n",
        "    \"\"\"\n",
        "    Validate the provided Gemini API key by configuring the genai client and\n",
        "    testing a content generation request.\n",
        "\n",
        "    Args:\n",
        "        my_api_key (str): The Gemini API key to be validated.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the API key is valid and the content generation request\n",
        "              is successful, False otherwise.\n",
        "\n",
        "    This function performs the following steps:\n",
        "        1. Configures the genai client with the provided API key.\n",
        "        2. Attempts to create a GenerativeModel instance and generate content\n",
        "           to validate the API key.\n",
        "        3. Returns True if successful; otherwise, displays the error message\n",
        "           and returns False.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        genai.configure(api_key=my_api_key)\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "        model.generate_content(\"What's my name?\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def generate_reccommendations():\n",
        "    \"\"\"\n",
        "    Generate recommended background places for the user's product using the\n",
        "    Gemini API.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of recommended background places for the product.\n",
        "\n",
        "    This function performs the following steps:\n",
        "        1. Retrieves the Gemini API key and product name from the session state.\n",
        "        2. Configures the genai client with the API key.\n",
        "        3. Creates a GenerativeModel instance and constructs a prompt to generate\n",
        "           recommendations.\n",
        "        4. Sends the prompt to the model and parses the JSON response to extract\n",
        "           the list of recommended places.\n",
        "    \"\"\"\n",
        "    gemini_api_key = st.session_state.api_key\n",
        "    product_name = st.session_state.product_name\n",
        "    genai.configure(api_key=gemini_api_key)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "    prompt = f'What are some great recommended background places for a {product_name}?'\n",
        "    prompt += 'Give me the answer as a list of places. There should be 5 relevant places.'\n",
        "    prompt += 'The answer\\'s format should be as follows: \\'[\"on a beach\", \"in a park\", \"near a flowing river\"]\\'.'\n",
        "    prompt += 'You need to follow this answer\\'s format strictly. Don\\'t ask more questions.'\n",
        "    response = model.generate_content(prompt)\n",
        "    return json.loads(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "GGaUmeL0zbpa",
        "outputId": "976e9d80-eaef-4c05-d324-b80816fa237d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gemini_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BaW0Yh2HZm9",
        "outputId": "f94c397d-c491-4b85-a62e-5c77dee05baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "#@title Writing main.py\n",
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "import os\n",
        "from train_model import train_model\n",
        "import shutil\n",
        "from generate_image import generate_image\n",
        "from gemini_api import validate_gemini_api_key, generate_reccommendations\n",
        "\n",
        "# Main Application with GUI\n",
        "\n",
        "\n",
        "def stage_0():\n",
        "    \"\"\"\n",
        "    Display the introductory page of the application.\n",
        "\n",
        "    This page explains the purpose of the application and provides a button to\n",
        "    start the creation process.\n",
        "    \"\"\"\n",
        "    st.write(\"Ready to showcase your product in its best light, but lacking professional photography tools?\")\n",
        "    st.write(\"Let our AI do the work for you.\")\n",
        "    st.write(\"Craft exquisite, professional-grade product images effortlessly, and at no cost.\")\n",
        "    st.write(\"Transform your brand's visual presentation with ease.\")\n",
        "    if st.button(\"Start Creating\"):\n",
        "        st.session_state.current_stage = 1\n",
        "        st.rerun()  # Move to next page\n",
        "\n",
        "\n",
        "def stage_1():\n",
        "    \"\"\"\n",
        "    Display the API key input page.\n",
        "\n",
        "    This page instructs the user to create a Gemini API key and input it into\n",
        "    the text box provided. The key is validated, and the user is moved to the\n",
        "    next stage if the key is valid.\n",
        "    \"\"\"\n",
        "    st.write(\"To start, go to [this link](https://aistudio.google.com/app/apikey)\"\n",
        "             + \" to create your Gemini API key (Don't worry, it's free)\")\n",
        "    api_key = st.text_input(\"Enter your Google Gemini API Key here:\")\n",
        "    if api_key:\n",
        "        if not validate_gemini_api_key(api_key):\n",
        "            st.error(\"Not the correct API key\")\n",
        "        else:\n",
        "            st.session_state.current_stage = 2\n",
        "            st.session_state.api_key = api_key\n",
        "            st.rerun()  # Move to next page\n",
        "\n",
        "\n",
        "def stage_2():\n",
        "    \"\"\"\n",
        "    Display the image upload page.\n",
        "\n",
        "    This page allows the user to upload 5-10 photos of their product. The images\n",
        "    are stored in a directory for later use in model training.\n",
        "    \"\"\"\n",
        "    st.write(\"Now upload 5-10 photos of your product, it can be photos taken of your product from different angles\")\n",
        "    files = st.file_uploader(\n",
        "        \"Upload images of your product:\",\n",
        "        type=[\"jpg\", \"jpeg\"],\n",
        "        accept_multiple_files=True,\n",
        "        help=\"Upload here...\")\n",
        "\n",
        "    if files:\n",
        "        if os.path.exists(\"./training_photos/\"):\n",
        "            shutil.rmtree(\"./training_photos/\")  # Create the folder for photos storage\n",
        "        os.makedirs(\"./training_photos/\")\n",
        "        for i, file in enumerate(files):\n",
        "            with open(os.path.join(\"./training_photos/\", f\"image_{i}.jpg\"), \"wb\") as f:\n",
        "                f.write(file.getbuffer())\n",
        "        st.session_state.current_stage = 3\n",
        "        st.rerun()  # Move to next page\n",
        "\n",
        "\n",
        "def stage_3():\n",
        "    \"\"\"\n",
        "    Display the product name input page.\n",
        "\n",
        "    This page prompts the user to enter the name of their product, which will\n",
        "    be used in model training and image generation.\n",
        "    \"\"\"\n",
        "    st.write(\"What's your product's name?\")\n",
        "    st.write(\"E.g. corggi dog, champions league soccer ball, red scarf with patterns\")\n",
        "    product_name = st.text_input(\"Enter your product's name here:\")\n",
        "    if product_name:\n",
        "        st.session_state.current_stage = 4\n",
        "        st.session_state.product_name = product_name\n",
        "        st.rerun()  # Move to next page\n",
        "\n",
        "\n",
        "def stage_4():\n",
        "    \"\"\"\n",
        "    Display the model training page.\n",
        "\n",
        "    This page initiates the model training process using the uploaded images and\n",
        "    product name. A progress bar is shown to indicate the training status.\n",
        "    \"\"\"\n",
        "    st.write(\"Now it's time to train your model.\")\n",
        "    st.write(\"Please wait a while for us to understand your product.\")\n",
        "    product_name = st.session_state.product_name\n",
        "    my_bar = st.progress(0, text=\"Training in progress...\")  # Monitor the progress\n",
        "    try:\n",
        "        train_model(product_name, my_bar)  # Finetuning the model\n",
        "        st.session_state.current_stage = 5\n",
        "        st.rerun()  # Move to next page\n",
        "    except Exception as e:\n",
        "        st.error(e)\n",
        "\n",
        "\n",
        "def stage_5():\n",
        "    \"\"\"\n",
        "    Display the background selection page.\n",
        "\n",
        "    This page allows the user to select or enter a background for the generated\n",
        "    product images. Background recommendations are provided for convenience.\n",
        "    \"\"\"\n",
        "    st.write(\"Everything's finally done, Hooray!\")\n",
        "    st.write(\"Now's the time to create stunning photos with your product.\")\n",
        "    st.write(\"Enter a background you want or choose a reccommendation below\")\n",
        "    if \"recs\" not in st.session_state:\n",
        "        st.session_state.recs = generate_reccommendations()  # Create some background recommendations\n",
        "    for rec in st.session_state.recs:\n",
        "        if st.button(rec):\n",
        "            st.session_state.current_stage = 6\n",
        "            st.session_state.background = rec\n",
        "            st.rerun()  # Move to next page\n",
        "    text_input = st.text_input(\"Enter a background: \")\n",
        "    if text_input:\n",
        "        st.session_state.current_stage = 6\n",
        "        st.session_state.background = text_input\n",
        "        st.rerun()  # Move to next page\n",
        "\n",
        "\n",
        "def stage_6():\n",
        "    \"\"\"\n",
        "    Display the image generation and results page.\n",
        "\n",
        "    This page generates and displays an image of the product with the selected\n",
        "    background. Users can generate another image or choose a different background.\n",
        "    \"\"\"\n",
        "    st.write(\"Nice Choice!\")\n",
        "    product_name = st.session_state.product_name\n",
        "    background = st.session_state.background\n",
        "    prompt = f\"a photo of a {product_name} {background}\"\n",
        "    st.write(f\"Finally, here's your {product_name} {background}\")\n",
        "    image = generate_image(prompt)\n",
        "    st.image(image)\n",
        "    if st.button(\"Generate another image\"):\n",
        "        st.rerun()  # Rerun stage 6\n",
        "    if st.button(\"Choose another background\"):\n",
        "        st.session_state.current_stage = 5\n",
        "        st.rerun()  # Return to stage 5\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":  # Main page and page control system\n",
        "    _ = \"\"\"\n",
        "    Main function to control the page flow of the application.\n",
        "\n",
        "    This function checks the current stage stored in the session state and\n",
        "    displays the appropriate page based on the stage.\n",
        "    \"\"\"\n",
        "    st.title(\"AI-Powered Product Photoshoot Wizard\")\n",
        "\n",
        "    if \"current_stage\" not in st.session_state:\n",
        "        st.session_state.current_stage = 0\n",
        "\n",
        "    curr_stage = st.session_state.current_stage\n",
        "\n",
        "    if curr_stage == 0:\n",
        "        stage_0()\n",
        "\n",
        "    elif curr_stage == 1:\n",
        "        stage_1()\n",
        "\n",
        "    elif curr_stage == 2:\n",
        "        stage_2()\n",
        "\n",
        "    elif curr_stage == 3:\n",
        "        stage_3()\n",
        "\n",
        "    elif curr_stage == 4:\n",
        "        stage_4()\n",
        "\n",
        "    elif curr_stage == 5:\n",
        "        stage_5()\n",
        "\n",
        "    elif curr_stage == 6:\n",
        "        stage_6()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ESuBiTXOh69",
        "outputId": "415ef9fd-de19-4fe8-f874-271096a5eaf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://d56b-34-127-91-106.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import conf, ngrok\n",
        "\n",
        "conf.get_default().region = \"us\"\n",
        "ngrok_key = \"2gssWVZmNGCsLcs8n3NAulvIqYE_5DupK4SWUHnGbWjAe7WAw\"\n",
        "conf.get_default().auth_token = ngrok_key\n",
        "port = 8501\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbvBeeEmO1yZ",
        "outputId": "f99455b6-4963-4df6-e5fa-e3f92b617864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.127.91.106:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run main.py &"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOTEb+ZRlJv1hKcV5vvYsx+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}